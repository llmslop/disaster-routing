# Minimum-Cost Flow-Based Evolutionary Optimization for Disaster-Resilient Routing in Elastic Optical Inter-Data Center Networks

This is the implementation reposistory of the paper _Minimum-Cost Flow-Based
Evolutionary Optimization for Disaster-Resilient Routing in Elastic Optical
Inter-Data Center Networks_, containing test instances and a Python
implementation.

## Building

[uv](https://docs.astral.sh/uv/) is used for dependency management, but it is
possible to use pip directly. Install the dependencies in `pyproject.toml`
(which comes with two optional dependency groups enabled by default: `color` for
colored terminal output, and `dev` for scripts). Dependencies are installed
automatically on first run, but it is possible to install them manually with:

```bash
uv sync
```

Additionally, a native module `dr_native` written in Rust is used for
performance-critical code. Currently, it has an alternative stochastic FPGA
implementation, which is faster than the raw Python version due to it containing
the heap `log N` optimization and being written in a compiled language.
Interfacing between the main Python module and the Rust module is done via
[PyO3](https://pyo3.rs/).

To build the native module, you need to have a Rust toolchain installed. Then,
run the following

```bash
# change directory to the native module
cd dr_native
# build and install the module in "develop" mode
uv tool run maturin develop --release
```

> [!NOTE]
> Experiments done in our papers all use the Rust module (both heuristics and
> SGA). This ensures fair comparison between methods. The speed improvement is
> solely to get around GitHub Actions' 6 hour time limits.
>
> The pure Python and Rust implementations might not give identical results due
> to unambiguous tie breaking.

## Running

The test instances used in the experiments of our paper are provided in the
`instances/dataset-1` directory. They are randomly generated by
`scripts/gen_instance.py`, each being a JSON file containing the network
topology, disaster zones, data center placements, and request data.

Our code uses `hydra` extensively for configuration management. To run an
experiment, specify options via the command line or a config file. It is also
important to note that we only support running in module mode, i.e., via

```bash
uv run  -m disaster_routing.main\
         instance.path=instances/dataset-1/cost239-0010-00.json\
        +solver=two_phase\
            +solver.dsa_solver=npm\
            +solver.router=greedy\
         hydra.verbose=true
```

More specifics on the options are provided below. The CI file
`.github/workflows/ci.yml` provides the specific commands used in the paper's
experiments.

### RNG seeding

> [!NOTE]
> All RNGs are seeded with 42 by default for reproducibility.

Many configurations will feature a `random` property, which can be set to
`seeded` or `unseeded`. If `seeded`, the seed can be customized further by
setting `random.seed`.

However, this configuration system is currently still broken, as some
combination of configs could give unexpected errors. It does not affect our
experimental results, nevertheless.

### Instance configuration

The default instance path is `instances/temp_instance.json`. To change it,
override the `instance.path` option like in the example above. If the path
points to a non-existent file, a new instance will be generated according to
the provided (or default) instance generation options.

Instance generation options are provided in `instance.*`, which include:

- `instance.num_requests: int`: number of generated requests
- `instance.topology: str`: topology name
  e.g., `cost239`, `nsfnet` and `usb`
- `instance.possible_dc_positions: list[int]`: list of possible DC node IDs
- `instance.content_count: int`: number of generated contents
- `instance.dc_count: int`: (maximum) number of DC nodes
- `instance.transmission_rate_range: tuple[int, int]`: range of request
  transmission rates
- `instance.force_recreate: bool`: whether to force recreate the instance even
  if the file `instance.path` already exists

Most of these instance generation flags are used by `scripts/gen_instance.py`,
but it is possible to use those for manual instance generation.

### DC placement configuration

In our design, DC placement is separated from the routing and DSA solving
process. Specify the placement configuration at `content_placement`.
We provide the following basic placement strategies:

- `naive`: simply placing content at the first `dc_count` DCs in the DC node
  list. This is mainly used as a base implementation for the other strategies
- `stochastic`: content are randomly placed. Configure the RNG via
  `content_placement.random`.
- `greedy`: equivalent to solving the reduced ILP for content placement,
  minimizing the average hops to DCs.

### Evaluation configuration

Since the CDP problem is a dual-objective optimization problem, a scalarization
process is necessary to produce a singular objective. The approach used in both
the original paper and our paper is the classical weighted-sum approach, however
there are many ways to set the weight values:

- `evaluation=weighted_sum`: The user passes the weight value directly. Note that
  if only one of the weights of MOFI/total FS is passed, the other weight will
  be automatically deduced via `mofi_weight + total_fs_weight = 1`.
- `evaluation=relative`: The user passes the same weight value as the above, but
  an initial run will be used to automatically normalize both objectives. The
  user must pass in configurations for the base run router and DSA solver.

### Solver configuration

We follow a composition-based solver configuration scheme. A solver is
responsible for both routing and DSA solving.

**Two phase solvers** are solvers that handle these two steps separately.
Configure these via specifying the router and DSA solver config:

```bash
[uv run ...] +solver=two_phase\
             +solver.router=greedy\
             +solver.dsa_solver=npm
```

We have two dedicated routers (greedy and flow) and a number of DSA solver
strategies (FPGA, NPM, GA).

**LS solver** can be constructed from any solver, and it adds a backtracking
phase after solving. This mirrors the HCDP backtracking step, which removes MOFI
edges from the graph until no improvements can be made.

For example, here is the configuration used for HCDP

```bash
[uv run ...] +solver=ls
                 +solver.base=two_phase\
                     +solver.base.router=greedy\
                     +solver.base.dsa_solver=npm\
                 +solver.f_max = 100 # F_max in the backtracking algorithm
```

Flow-CDP in our paper has a similar configuration, with the only change being
the router

```bash
[uv run ...] +solver=ls
                 +solver.base=two_phase\
                     +solver.base.router=flow\
                     +solver.base.dsa_solver=npm\
                 +solver.f_max = 100 # F_max in the backtracking algorithm
```

The `ilp` solver simply solves the ILP model. We are unable to reproduce the
results in the original paper under any reasonable time frame using the CBC
solver, so we did not include any ILP results in the paper.

The `sga` solver is our proposed GA-CDP algorithm. Since using the main DSA
solver for fitness evaluation could be costly, we provide a
`solver.approximate_dsa_solver` option. Hybrid initialization can be disabled via
`+solver.hybrid_init=false`. The GA-CDP algorithm in our paper is configured as

```bash
[uv run ...] +solver=sga
                 +solver.approximate_dsa_solver=fpga\
                 +solver.dsa_solver=npm\
                 +solver.hybrid_init=true
```

Solutions produced can be automatically converted into an ILP solution for
verification. This process is done automatically for smaller-scale instances (at
most 50 requests), but can be manually enabled via the (global) `ilp_check`
option.

## Scripts

Scripts (for post-run data analysis, plotting, etc.) are provided in the
`scripts/` directory. See `scripts/README.md` (quite outdated) for more details.
